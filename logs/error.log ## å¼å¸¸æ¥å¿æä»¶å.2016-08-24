2016-08-24 15:25:33  [ main:0 ] - [ INFO ]  Obtained 6791 TLD from packaged file tld-names.txt
2016-08-24 15:25:33  [ main:65 ] - [ INFO ]  Deleted contents of: ./crawl_storage/frontier ( as you have configured resumable crawling to false )
2016-08-24 15:25:35  [ main:1593 ] - [ INFO ]  Crawler 1 started
2016-08-24 15:25:35  [ main:1593 ] - [ INFO ]  Crawler 2 started
2016-08-24 15:25:35  [ main:1593 ] - [ INFO ]  Crawler 3 started
2016-08-24 15:25:35  [ main:1593 ] - [ INFO ]  Crawler 4 started
2016-08-24 15:25:35  [ main:1594 ] - [ INFO ]  Crawler 5 started
2016-08-24 15:25:35  [ main:1594 ] - [ INFO ]  Crawler 6 started
2016-08-24 15:25:35  [ main:1594 ] - [ INFO ]  Crawler 7 started
2016-08-24 15:25:35  [ main:1594 ] - [ INFO ]  Crawler 8 started
2016-08-24 15:25:35  [ main:1594 ] - [ INFO ]  Crawler 9 started
2016-08-24 15:25:35  [ main:1594 ] - [ INFO ]  Crawler 10 started
2016-08-24 15:25:45  [ Thread-1:11599 ] - [ INFO ]  It looks like no thread is working, waiting for 10 seconds to make sure...
2016-08-24 15:25:55  [ Thread-1:21610 ] - [ INFO ]  No thread is working and no more URLs are in queue waiting for another 10 seconds to make sure...
2016-08-24 15:26:05  [ Thread-1:31613 ] - [ INFO ]  All of the crawlers are stopped. Finishing the process...
2016-08-24 15:26:05  [ Thread-1:31613 ] - [ INFO ]  Waiting for 10 seconds before final clean up...
2016-08-24 15:29:12  [ main:0 ] - [ INFO ]  Obtained 6791 TLD from packaged file tld-names.txt
2016-08-24 15:29:12  [ main:29 ] - [ INFO ]  Deleted contents of: ./crawl_storage/frontier ( as you have configured resumable crawling to false )
2016-08-24 15:29:18  [ main:5494 ] - [ INFO ]  Crawler 1 started
2016-08-24 15:29:18  [ main:5494 ] - [ INFO ]  Crawler 2 started
2016-08-24 15:29:18  [ main:5495 ] - [ INFO ]  Crawler 3 started
2016-08-24 15:29:18  [ main:5495 ] - [ INFO ]  Crawler 4 started
2016-08-24 15:29:18  [ main:5495 ] - [ INFO ]  Crawler 5 started
2016-08-24 15:29:28  [ Thread-1:15499 ] - [ INFO ]  It looks like no thread is working, waiting for 10 seconds to make sure...
2016-08-24 15:29:38  [ Thread-1:25506 ] - [ INFO ]  No thread is working and no more URLs are in queue waiting for another 10 seconds to make sure...
2016-08-24 15:29:48  [ Thread-1:35508 ] - [ INFO ]  All of the crawlers are stopped. Finishing the process...
2016-08-24 15:29:48  [ Thread-1:35509 ] - [ INFO ]  Waiting for 10 seconds before final clean up...
2016-08-24 15:32:53  [ main:1 ] - [ INFO ]  Obtained 6791 TLD from packaged file tld-names.txt
2016-08-24 15:32:53  [ main:29 ] - [ INFO ]  Deleted contents of: ./crawl_storage/frontier ( as you have configured resumable crawling to false )
2016-08-24 15:33:00  [ main:7652 ] - [ INFO ]  Crawler 1 started
2016-08-24 15:33:00  [ main:7652 ] - [ INFO ]  Crawler 2 started
2016-08-24 15:33:00  [ main:7652 ] - [ INFO ]  Crawler 3 started
2016-08-24 15:33:00  [ main:7652 ] - [ INFO ]  Crawler 4 started
2016-08-24 15:33:00  [ main:7653 ] - [ INFO ]  Crawler 5 started
2016-08-24 15:33:10  [ Thread-1:17654 ] - [ INFO ]  It looks like no thread is working, waiting for 10 seconds to make sure...
2016-08-24 15:33:20  [ Thread-1:27665 ] - [ INFO ]  No thread is working and no more URLs are in queue waiting for another 10 seconds to make sure...
2016-08-24 15:33:30  [ Thread-1:37666 ] - [ INFO ]  All of the crawlers are stopped. Finishing the process...
2016-08-24 15:33:30  [ Thread-1:37666 ] - [ INFO ]  Waiting for 10 seconds before final clean up...
2016-08-24 15:36:53  [ main:0 ] - [ INFO ]  Obtained 6791 TLD from packaged file tld-names.txt
2016-08-24 15:36:53  [ main:33 ] - [ INFO ]  Deleted contents of: ./crawl_storage/frontier ( as you have configured resumable crawling to false )
2016-08-24 15:36:54  [ main:909 ] - [ INFO ]  Crawler 1 started
2016-08-24 15:36:54  [ main:910 ] - [ INFO ]  Crawler 2 started
2016-08-24 15:36:54  [ main:910 ] - [ INFO ]  Crawler 3 started
2016-08-24 15:36:54  [ main:910 ] - [ INFO ]  Crawler 4 started
2016-08-24 15:36:54  [ main:910 ] - [ INFO ]  Crawler 5 started
2016-08-24 15:37:14  [ Thread-1:20917 ] - [ INFO ]  It looks like no thread is working, waiting for 10 seconds to make sure...
2016-08-24 15:37:24  [ Thread-1:30924 ] - [ INFO ]  No thread is working and no more URLs are in queue waiting for another 10 seconds to make sure...
2016-08-24 15:37:34  [ Thread-1:40926 ] - [ INFO ]  All of the crawlers are stopped. Finishing the process...
2016-08-24 15:37:34  [ Thread-1:40927 ] - [ INFO ]  Waiting for 10 seconds before final clean up...
2016-08-24 15:42:17  [ main:0 ] - [ INFO ]  Obtained 6791 TLD from packaged file tld-names.txt
2016-08-24 15:42:17  [ main:34 ] - [ INFO ]  Deleted contents of: ./crawl_storage/frontier ( as you have configured resumable crawling to false )
2016-08-24 15:42:30  [ main:13610 ] - [ INFO ]  Crawler 1 started
2016-08-24 15:42:30  [ main:13611 ] - [ INFO ]  Crawler 2 started
2016-08-24 15:42:30  [ main:13611 ] - [ INFO ]  Crawler 3 started
2016-08-24 15:42:30  [ main:13611 ] - [ INFO ]  Crawler 4 started
2016-08-24 15:42:30  [ main:13611 ] - [ INFO ]  Crawler 5 started
2016-08-24 15:43:03  [ main:0 ] - [ INFO ]  Obtained 6791 TLD from packaged file tld-names.txt
2016-08-24 15:43:03  [ main:30 ] - [ INFO ]  Deleted contents of: ./crawl_storage/frontier ( as you have configured resumable crawling to false )
2016-08-24 15:43:04  [ main:590 ] - [ INFO ]  Crawler 1 started
2016-08-24 15:43:04  [ main:591 ] - [ INFO ]  Crawler 2 started
2016-08-24 15:43:04  [ main:591 ] - [ INFO ]  Crawler 3 started
2016-08-24 15:43:04  [ main:591 ] - [ INFO ]  Crawler 4 started
2016-08-24 15:43:04  [ main:591 ] - [ INFO ]  Crawler 5 started
2016-08-24 15:43:14  [ Thread-1:10593 ] - [ INFO ]  It looks like no thread is working, waiting for 10 seconds to make sure...
2016-08-24 15:44:03  [ main:0 ] - [ INFO ]  Obtained 6791 TLD from packaged file tld-names.txt
2016-08-24 15:44:03  [ main:29 ] - [ INFO ]  Deleted contents of: ./crawl_storage/frontier ( as you have configured resumable crawling to false )
2016-08-24 15:44:04  [ main:931 ] - [ INFO ]  Crawler 1 started
2016-08-24 15:44:04  [ main:931 ] - [ INFO ]  Crawler 2 started
2016-08-24 15:44:04  [ main:932 ] - [ INFO ]  Crawler 3 started
2016-08-24 15:44:04  [ main:932 ] - [ INFO ]  Crawler 4 started
2016-08-24 15:44:04  [ main:932 ] - [ INFO ]  Crawler 5 started
2016-08-24 15:44:14  [ Thread-1:10937 ] - [ INFO ]  It looks like no thread is working, waiting for 10 seconds to make sure...
2016-08-24 15:44:24  [ Thread-1:20947 ] - [ INFO ]  No thread is working and no more URLs are in queue waiting for another 10 seconds to make sure...
2016-08-24 15:44:34  [ Thread-1:30949 ] - [ INFO ]  All of the crawlers are stopped. Finishing the process...
2016-08-24 15:44:34  [ Thread-1:30950 ] - [ INFO ]  Waiting for 10 seconds before final clean up...
2016-08-24 15:53:32  [ main:0 ] - [ INFO ]  Obtained 6791 TLD from packaged file tld-names.txt
2016-08-24 15:53:32  [ main:39 ] - [ INFO ]  Deleted contents of: ./crawl_storage/frontier ( as you have configured resumable crawling to false )
2016-08-24 15:53:38  [ main:5992 ] - [ INFO ]  Crawler 1 started
2016-08-24 15:53:38  [ main:5992 ] - [ INFO ]  Crawler 2 started
2016-08-24 15:53:38  [ main:5992 ] - [ INFO ]  Crawler 3 started
2016-08-24 15:53:38  [ main:5992 ] - [ INFO ]  Crawler 4 started
2016-08-24 15:53:38  [ main:5993 ] - [ INFO ]  Crawler 5 started
2016-08-24 15:53:48  [ Thread-1:15997 ] - [ INFO ]  It looks like no thread is working, waiting for 10 seconds to make sure...
2016-08-24 15:53:58  [ Thread-1:26008 ] - [ INFO ]  No thread is working and no more URLs are in queue waiting for another 10 seconds to make sure...
2016-08-24 15:54:08  [ Thread-1:36008 ] - [ INFO ]  All of the crawlers are stopped. Finishing the process...
2016-08-24 15:54:08  [ Thread-1:36009 ] - [ INFO ]  Waiting for 10 seconds before final clean up...
2016-08-24 15:57:12  [ main:0 ] - [ INFO ]  Obtained 6791 TLD from packaged file tld-names.txt
2016-08-24 15:57:12  [ main:31 ] - [ INFO ]  Deleted contents of: ./crawl_storage/frontier ( as you have configured resumable crawling to false )
2016-08-24 15:57:13  [ main:449 ] - [ INFO ]  Crawler 1 started
2016-08-24 15:57:13  [ main:450 ] - [ INFO ]  Crawler 2 started
2016-08-24 15:57:13  [ main:450 ] - [ INFO ]  Crawler 3 started
2016-08-24 15:57:13  [ main:450 ] - [ INFO ]  Crawler 4 started
2016-08-24 15:57:13  [ main:450 ] - [ INFO ]  Crawler 5 started
2016-08-24 15:57:23  [ Thread-1:10456 ] - [ INFO ]  It looks like no thread is working, waiting for 10 seconds to make sure...
2016-08-24 15:57:33  [ Thread-1:20466 ] - [ INFO ]  No thread is working and no more URLs are in queue waiting for another 10 seconds to make sure...
2016-08-24 15:57:43  [ Thread-1:30471 ] - [ INFO ]  All of the crawlers are stopped. Finishing the process...
2016-08-24 15:57:43  [ Thread-1:30471 ] - [ INFO ]  Waiting for 10 seconds before final clean up...
2016-08-24 16:02:05  [ main:0 ] - [ INFO ]  Obtained 6791 TLD from packaged file tld-names.txt
2016-08-24 16:02:05  [ main:29 ] - [ INFO ]  Deleted contents of: ./crawl_storage/frontier ( as you have configured resumable crawling to false )
2016-08-24 16:02:14  [ main:8334 ] - [ INFO ]  Crawler 1 started
2016-08-24 16:02:14  [ main:8334 ] - [ INFO ]  Crawler 2 started
2016-08-24 16:02:14  [ main:8335 ] - [ INFO ]  Crawler 3 started
2016-08-24 16:02:14  [ main:8335 ] - [ INFO ]  Crawler 4 started
2016-08-24 16:02:14  [ main:8335 ] - [ INFO ]  Crawler 5 started
2016-08-24 16:04:18  [ Crawler 1:132523 ] - [ ERROR ]  Error occurred while fetching (robots) url: http://www.bloomberg.com/robots.txt
org.apache.http.conn.ConnectTimeoutException: Connect to www.bloomberg.com:80 [www.bloomberg.com/93.46.8.89] failed: connect timed out
	at org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:150)
	at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:353)
	at org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:380)
	at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236)
	at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:184)
	at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:88)
	at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110)
	at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:184)
	at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)
	at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:107)
	at edu.uci.ics.crawler4j.fetcher.PageFetcher.fetchPage(PageFetcher.java:237)
	at edu.uci.ics.crawler4j.robotstxt.RobotstxtServer.fetchDirectives(RobotstxtServer.java:100)
	at edu.uci.ics.crawler4j.robotstxt.RobotstxtServer.allows(RobotstxtServer.java:80)
	at edu.uci.ics.crawler4j.crawler.CrawlController.addSeed(CrawlController.java:427)
	at edu.uci.ics.crawler4j.crawler.CrawlController.addSeed(CrawlController.java:381)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketTimeoutException: connect timed out
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at org.apache.http.conn.socket.PlainConnectionSocketFactory.connectSocket(PlainConnectionSocketFactory.java:74)
	at org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:141)
	... 18 more
2016-08-24 16:16:16  [ main:0 ] - [ INFO ]  Obtained 6791 TLD from packaged file tld-names.txt
2016-08-24 16:16:16  [ main:37 ] - [ INFO ]  Deleted contents of: ./crawl_storage/frontier ( as you have configured resumable crawling to false )
2016-08-24 16:16:16  [ main:523 ] - [ INFO ]  Crawler 1 started
2016-08-24 16:16:16  [ main:524 ] - [ INFO ]  Crawler 2 started
2016-08-24 16:16:16  [ main:524 ] - [ INFO ]  Crawler 3 started
2016-08-24 16:16:16  [ main:524 ] - [ INFO ]  Crawler 4 started
2016-08-24 16:16:16  [ main:524 ] - [ INFO ]  Crawler 5 started
2016-08-24 16:16:26  [ Thread-1:10530 ] - [ INFO ]  It looks like no thread is working, waiting for 10 seconds to make sure...
2016-08-24 16:16:36  [ Thread-1:20537 ] - [ INFO ]  No thread is working and no more URLs are in queue waiting for another 10 seconds to make sure...
2016-08-24 16:16:46  [ Thread-1:30541 ] - [ INFO ]  All of the crawlers are stopped. Finishing the process...
2016-08-24 16:16:46  [ Thread-1:30541 ] - [ INFO ]  Waiting for 10 seconds before final clean up...
2016-08-24 17:08:03  [ main:1 ] - [ INFO ]  Obtained 6791 TLD from packaged file tld-names.txt
2016-08-24 17:08:03  [ main:39 ] - [ INFO ]  Deleted contents of: ./crawl_storage/frontier ( as you have configured resumable crawling to false )
2016-08-24 17:08:06  [ main:2850 ] - [ INFO ]  Crawler 1 started
2016-08-24 17:08:06  [ main:2850 ] - [ INFO ]  Crawler 2 started
2016-08-24 17:08:06  [ main:2851 ] - [ INFO ]  Crawler 3 started
2016-08-24 17:08:06  [ main:2851 ] - [ INFO ]  Crawler 4 started
2016-08-24 17:08:06  [ main:2851 ] - [ INFO ]  Crawler 5 started
2016-08-24 17:08:16  [ Thread-1:12853 ] - [ INFO ]  It looks like no thread is working, waiting for 10 seconds to make sure...
2016-08-24 17:08:26  [ Thread-1:22862 ] - [ INFO ]  No thread is working and no more URLs are in queue waiting for another 10 seconds to make sure...
2016-08-24 17:08:36  [ Thread-1:32865 ] - [ INFO ]  All of the crawlers are stopped. Finishing the process...
2016-08-24 17:08:36  [ Thread-1:32865 ] - [ INFO ]  Waiting for 10 seconds before final clean up...
2016-08-24 17:28:18  [ main:0 ] - [ INFO ]  Obtained 6791 TLD from packaged file tld-names.txt
2016-08-24 17:28:18  [ main:30 ] - [ INFO ]  Deleted contents of: ./crawl_storage/frontier ( as you have configured resumable crawling to false )
2016-08-24 17:28:27  [ main:0 ] - [ INFO ]  Obtained 6791 TLD from packaged file tld-names.txt
2016-08-24 17:28:27  [ main:31 ] - [ INFO ]  Deleted contents of: ./crawl_storage/frontier ( as you have configured resumable crawling to false )
2016-08-24 17:28:28  [ main:842 ] - [ INFO ]  Crawler 1 started
2016-08-24 17:28:28  [ main:843 ] - [ INFO ]  Crawler 2 started
2016-08-24 17:28:28  [ main:843 ] - [ INFO ]  Crawler 3 started
2016-08-24 17:28:28  [ main:843 ] - [ INFO ]  Crawler 4 started
2016-08-24 17:28:28  [ main:843 ] - [ INFO ]  Crawler 5 started
2016-08-24 17:28:28  [ Crawler 1:1224 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/: null
2016-08-24 17:28:28  [ Crawler 1:1225 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:66)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:31)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 17:28:36  [ Crawler 2:9434 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/admin/index.php?action=index%2FtoolSubmit: null
2016-08-24 17:28:36  [ Crawler 2:9434 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:66)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:31)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 17:28:48  [ Thread-1:20849 ] - [ INFO ]  It looks like no thread is working, waiting for 10 seconds to make sure...
2016-08-24 17:28:58  [ Thread-1:30856 ] - [ INFO ]  No thread is working and no more URLs are in queue waiting for another 10 seconds to make sure...
2016-08-24 17:29:08  [ Thread-1:40860 ] - [ INFO ]  All of the crawlers are stopped. Finishing the process...
2016-08-24 17:29:08  [ Thread-1:40860 ] - [ INFO ]  Waiting for 10 seconds before final clean up...
2016-08-24 17:39:49  [ main:0 ] - [ INFO ]  Obtained 6791 TLD from packaged file tld-names.txt
2016-08-24 17:39:49  [ main:30 ] - [ INFO ]  Deleted contents of: ./crawl_storage/frontier ( as you have configured resumable crawling to false )
2016-08-24 17:39:50  [ main:540 ] - [ INFO ]  Crawler 1 started
2016-08-24 17:39:50  [ main:540 ] - [ INFO ]  Crawler 2 started
2016-08-24 17:39:50  [ main:540 ] - [ INFO ]  Crawler 3 started
2016-08-24 17:39:50  [ main:540 ] - [ INFO ]  Crawler 4 started
2016-08-24 17:39:50  [ main:541 ] - [ INFO ]  Crawler 5 started
2016-08-24 17:39:50  [ Crawler 1:1001 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/: null
2016-08-24 17:39:50  [ Crawler 1:1002 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:70)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 17:39:58  [ Crawler 2:9164 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/admin/index.php?action=index%2FtoolSubmit: null
2016-08-24 17:39:58  [ Crawler 2:9165 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:70)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 17:40:10  [ Thread-1:20545 ] - [ INFO ]  It looks like no thread is working, waiting for 10 seconds to make sure...
2016-08-24 17:40:20  [ Thread-1:30553 ] - [ INFO ]  No thread is working and no more URLs are in queue waiting for another 10 seconds to make sure...
2016-08-24 17:40:30  [ Thread-1:40557 ] - [ INFO ]  All of the crawlers are stopped. Finishing the process...
2016-08-24 17:40:30  [ Thread-1:40557 ] - [ INFO ]  Waiting for 10 seconds before final clean up...
2016-08-24 17:41:08  [ main:0 ] - [ INFO ]  Obtained 6791 TLD from packaged file tld-names.txt
2016-08-24 17:41:08  [ main:32 ] - [ INFO ]  Deleted contents of: ./crawl_storage/frontier ( as you have configured resumable crawling to false )
2016-08-24 17:41:14  [ main:5591 ] - [ INFO ]  Crawler 1 started
2016-08-24 17:41:29  [ Crawler 1:21150 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/: null
2016-08-24 17:41:29  [ Crawler 1:21150 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:70)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 17:42:59  [ main:0 ] - [ INFO ]  Obtained 6791 TLD from packaged file tld-names.txt
2016-08-24 17:42:59  [ main:40 ] - [ INFO ]  Deleted contents of: ./crawl_storage/frontier ( as you have configured resumable crawling to false )
2016-08-24 17:43:00  [ main:732 ] - [ INFO ]  Crawler 1 started
2016-08-24 17:43:00  [ Crawler 1:1361 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/: null
2016-08-24 17:43:00  [ Crawler 1:1362 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:70)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 17:53:02  [ main:0 ] - [ INFO ]  Obtained 6791 TLD from packaged file tld-names.txt
2016-08-24 17:53:02  [ main:33 ] - [ INFO ]  Deleted contents of: ./crawl_storage/frontier ( as you have configured resumable crawling to false )
2016-08-24 17:53:09  [ main:7312 ] - [ INFO ]  Crawler 1 started
2016-08-24 17:53:10  [ Crawler 1:7706 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/: null
2016-08-24 17:53:10  [ Crawler 1:7707 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:70)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 17:53:10  [ Crawler 1:7817 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/yxgj.html: null
2016-08-24 17:53:10  [ Crawler 1:7817 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at org.htmlparser.nodes.TagNode.getAttributeEx(TagNode.java:303)
	at org.htmlparser.nodes.TagNode.getAttribute(TagNode.java:174)
	at spider.util.ParseUtils$1.accept(ParseUtils.java:41)
	at org.htmlparser.nodes.AbstractNode.collectInto(AbstractNode.java:192)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:433)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.Parser.parse(Parser.java:702)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:33)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:106)
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:74)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 17:53:10  [ Crawler 1:7975 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/wlqs.html: null
2016-08-24 17:53:10  [ Crawler 1:7975 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at org.htmlparser.nodes.TagNode.getAttributeEx(TagNode.java:303)
	at org.htmlparser.nodes.TagNode.getAttribute(TagNode.java:174)
	at spider.util.ParseUtils$1.accept(ParseUtils.java:41)
	at org.htmlparser.nodes.AbstractNode.collectInto(AbstractNode.java:192)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:433)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.Parser.parse(Parser.java:702)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:33)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:106)
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:74)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 17:53:10  [ Crawler 1:8175 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/ncee.html: null
2016-08-24 17:53:10  [ Crawler 1:8175 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at org.htmlparser.nodes.TagNode.getAttributeEx(TagNode.java:303)
	at org.htmlparser.nodes.TagNode.getAttribute(TagNode.java:174)
	at spider.util.ParseUtils$1.accept(ParseUtils.java:41)
	at org.htmlparser.nodes.AbstractNode.collectInto(AbstractNode.java:192)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:433)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.Parser.parse(Parser.java:702)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:33)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:106)
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:74)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 17:53:10  [ Crawler 1:8389 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/wzpm.html: null
2016-08-24 17:53:10  [ Crawler 1:8389 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at org.htmlparser.nodes.TagNode.getAttributeEx(TagNode.java:303)
	at org.htmlparser.nodes.TagNode.getAttribute(TagNode.java:174)
	at spider.util.ParseUtils$1.accept(ParseUtils.java:41)
	at org.htmlparser.nodes.AbstractNode.collectInto(AbstractNode.java:192)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:433)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.Parser.parse(Parser.java:702)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:33)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:106)
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:74)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 17:53:10  [ Crawler 1:8580 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/p2p.html: null
2016-08-24 17:53:10  [ Crawler 1:8580 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at org.htmlparser.nodes.TagNode.getAttributeEx(TagNode.java:303)
	at org.htmlparser.nodes.TagNode.getAttribute(TagNode.java:174)
	at spider.util.ParseUtils$1.accept(ParseUtils.java:41)
	at org.htmlparser.nodes.AbstractNode.collectInto(AbstractNode.java:192)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:433)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.Parser.parse(Parser.java:702)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:33)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:106)
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:74)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 17:53:11  [ Crawler 1:8791 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/ypsj.html: null
2016-08-24 17:53:11  [ Crawler 1:8791 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at org.htmlparser.nodes.TagNode.getAttributeEx(TagNode.java:303)
	at org.htmlparser.nodes.TagNode.getAttribute(TagNode.java:174)
	at spider.util.ParseUtils$1.accept(ParseUtils.java:41)
	at org.htmlparser.nodes.AbstractNode.collectInto(AbstractNode.java:192)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:433)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.Parser.parse(Parser.java:702)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:33)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:106)
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:74)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 17:53:11  [ Crawler 1:8981 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/yhsj.html: null
2016-08-24 17:53:11  [ Crawler 1:8981 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at org.htmlparser.nodes.TagNode.getAttributeEx(TagNode.java:303)
	at org.htmlparser.nodes.TagNode.getAttribute(TagNode.java:174)
	at spider.util.ParseUtils$1.accept(ParseUtils.java:41)
	at org.htmlparser.nodes.AbstractNode.collectInto(AbstractNode.java:192)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:433)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.Parser.parse(Parser.java:702)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:33)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:106)
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:74)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 17:53:11  [ Crawler 1:9184 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/zxdc.html: null
2016-08-24 17:53:11  [ Crawler 1:9184 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at org.htmlparser.nodes.TagNode.getAttributeEx(TagNode.java:303)
	at org.htmlparser.nodes.TagNode.getAttribute(TagNode.java:174)
	at spider.util.ParseUtils$1.accept(ParseUtils.java:41)
	at org.htmlparser.nodes.AbstractNode.collectInto(AbstractNode.java:192)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:433)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.Parser.parse(Parser.java:702)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:33)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:106)
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:74)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 17:53:11  [ Crawler 1:9385 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/topindex.html: null
2016-08-24 17:53:11  [ Crawler 1:9385 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at org.htmlparser.nodes.TagNode.getAttributeEx(TagNode.java:303)
	at org.htmlparser.nodes.TagNode.getAttribute(TagNode.java:174)
	at spider.util.ParseUtils$1.accept(ParseUtils.java:41)
	at org.htmlparser.nodes.AbstractNode.collectInto(AbstractNode.java:192)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:433)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.Parser.parse(Parser.java:702)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:33)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:106)
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:74)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 17:53:11  [ Crawler 1:9590 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/aso.html: null
2016-08-24 17:53:11  [ Crawler 1:9590 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at org.htmlparser.nodes.TagNode.getAttributeEx(TagNode.java:303)
	at org.htmlparser.nodes.TagNode.getAttribute(TagNode.java:174)
	at spider.util.ParseUtils$1.accept(ParseUtils.java:41)
	at org.htmlparser.nodes.AbstractNode.collectInto(AbstractNode.java:192)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:433)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.Parser.parse(Parser.java:702)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:33)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:106)
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:74)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 17:53:12  [ Crawler 1:9796 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/brand.html: null
2016-08-24 17:53:12  [ Crawler 1:9796 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at org.htmlparser.nodes.TagNode.getAttributeEx(TagNode.java:303)
	at org.htmlparser.nodes.TagNode.getAttribute(TagNode.java:174)
	at spider.util.ParseUtils$1.accept(ParseUtils.java:41)
	at org.htmlparser.nodes.AbstractNode.collectInto(AbstractNode.java:192)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:433)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.Parser.parse(Parser.java:702)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:33)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:106)
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:74)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 17:53:12  [ Crawler 1:10005 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/social.html: null
2016-08-24 17:53:12  [ Crawler 1:10005 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at org.htmlparser.nodes.TagNode.getAttributeEx(TagNode.java:303)
	at org.htmlparser.nodes.TagNode.getAttribute(TagNode.java:174)
	at spider.util.ParseUtils$1.accept(ParseUtils.java:41)
	at org.htmlparser.nodes.AbstractNode.collectInto(AbstractNode.java:192)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:433)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.Parser.parse(Parser.java:702)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:33)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:106)
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:74)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 17:53:12  [ Crawler 1:10191 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/ydpm.html: null
2016-08-24 17:53:12  [ Crawler 1:10192 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at org.htmlparser.nodes.TagNode.getAttributeEx(TagNode.java:303)
	at org.htmlparser.nodes.TagNode.getAttribute(TagNode.java:174)
	at spider.util.ParseUtils$1.accept(ParseUtils.java:41)
	at org.htmlparser.nodes.AbstractNode.collectInto(AbstractNode.java:192)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:433)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.Parser.parse(Parser.java:702)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:33)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:106)
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:74)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 17:53:12  [ Crawler 1:10406 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/wzxn.html: null
2016-08-24 17:53:12  [ Crawler 1:10406 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at org.htmlparser.nodes.TagNode.getAttributeEx(TagNode.java:303)
	at org.htmlparser.nodes.TagNode.getAttribute(TagNode.java:174)
	at spider.util.ParseUtils$1.accept(ParseUtils.java:41)
	at org.htmlparser.nodes.AbstractNode.collectInto(AbstractNode.java:192)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:433)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.Parser.parse(Parser.java:702)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:33)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:106)
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:74)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 17:53:12  [ Crawler 1:10600 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/dxpm.html: null
2016-08-24 17:53:12  [ Crawler 1:10601 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at org.htmlparser.nodes.TagNode.getAttributeEx(TagNode.java:303)
	at org.htmlparser.nodes.TagNode.getAttribute(TagNode.java:174)
	at spider.util.ParseUtils$1.accept(ParseUtils.java:41)
	at org.htmlparser.nodes.AbstractNode.collectInto(AbstractNode.java:192)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:433)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.Parser.parse(Parser.java:702)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:33)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:106)
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:74)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 17:53:13  [ Crawler 1:10804 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/ylsj.html: null
2016-08-24 17:53:13  [ Crawler 1:10804 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at org.htmlparser.nodes.TagNode.getAttributeEx(TagNode.java:303)
	at org.htmlparser.nodes.TagNode.getAttribute(TagNode.java:174)
	at spider.util.ParseUtils$1.accept(ParseUtils.java:41)
	at org.htmlparser.nodes.AbstractNode.collectInto(AbstractNode.java:192)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:433)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.Parser.parse(Parser.java:702)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:33)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:106)
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:74)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 17:53:13  [ Crawler 1:11003 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/sssj.html: null
2016-08-24 17:53:13  [ Crawler 1:11003 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at org.htmlparser.nodes.TagNode.getAttributeEx(TagNode.java:303)
	at org.htmlparser.nodes.TagNode.getAttribute(TagNode.java:174)
	at spider.util.ParseUtils$1.accept(ParseUtils.java:41)
	at org.htmlparser.nodes.AbstractNode.collectInto(AbstractNode.java:192)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:433)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.Parser.parse(Parser.java:702)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:33)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:106)
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:74)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 17:53:13  [ Crawler 1:11208 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/jrsj.html: null
2016-08-24 17:53:13  [ Crawler 1:11208 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at org.htmlparser.nodes.TagNode.getAttributeEx(TagNode.java:303)
	at org.htmlparser.nodes.TagNode.getAttribute(TagNode.java:174)
	at spider.util.ParseUtils$1.accept(ParseUtils.java:41)
	at org.htmlparser.nodes.AbstractNode.collectInto(AbstractNode.java:192)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:433)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.Parser.parse(Parser.java:702)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:33)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:106)
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:74)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 17:53:13  [ Crawler 1:11415 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/ydcs.html: null
2016-08-24 17:53:13  [ Crawler 1:11415 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at org.htmlparser.nodes.TagNode.getAttributeEx(TagNode.java:303)
	at org.htmlparser.nodes.TagNode.getAttribute(TagNode.java:174)
	at spider.util.ParseUtils$1.accept(ParseUtils.java:41)
	at org.htmlparser.nodes.AbstractNode.collectInto(AbstractNode.java:192)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:433)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.Parser.parse(Parser.java:702)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:33)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:106)
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:74)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 17:53:13  [ Crawler 1:11624 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/ydyy.html: null
2016-08-24 17:53:13  [ Crawler 1:11624 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at org.htmlparser.nodes.TagNode.getAttributeEx(TagNode.java:303)
	at org.htmlparser.nodes.TagNode.getAttribute(TagNode.java:174)
	at spider.util.ParseUtils$1.accept(ParseUtils.java:41)
	at org.htmlparser.nodes.AbstractNode.collectInto(AbstractNode.java:192)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:433)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.Parser.parse(Parser.java:702)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:33)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:106)
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:74)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 17:53:14  [ Crawler 1:11816 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/top.html: null
2016-08-24 17:53:14  [ Crawler 1:11817 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at org.htmlparser.nodes.TagNode.getAttributeEx(TagNode.java:303)
	at org.htmlparser.nodes.TagNode.getAttribute(TagNode.java:174)
	at spider.util.ParseUtils$1.accept(ParseUtils.java:41)
	at org.htmlparser.nodes.AbstractNode.collectInto(AbstractNode.java:192)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:433)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.Parser.parse(Parser.java:702)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:33)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:106)
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:74)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 17:53:14  [ Crawler 1:12021 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/dssj.html: null
2016-08-24 17:53:14  [ Crawler 1:12021 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at org.htmlparser.nodes.TagNode.getAttributeEx(TagNode.java:303)
	at org.htmlparser.nodes.TagNode.getAttribute(TagNode.java:174)
	at spider.util.ParseUtils$1.accept(ParseUtils.java:41)
	at org.htmlparser.nodes.AbstractNode.collectInto(AbstractNode.java:192)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:433)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.Parser.parse(Parser.java:702)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:33)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:106)
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:74)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 17:53:14  [ Crawler 1:12218 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/tv.html: null
2016-08-24 17:53:14  [ Crawler 1:12218 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at org.htmlparser.nodes.TagNode.getAttributeEx(TagNode.java:303)
	at org.htmlparser.nodes.TagNode.getAttribute(TagNode.java:174)
	at spider.util.ParseUtils$1.accept(ParseUtils.java:41)
	at org.htmlparser.nodes.AbstractNode.collectInto(AbstractNode.java:192)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:433)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.Parser.parse(Parser.java:702)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:33)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:106)
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:74)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 17:53:53  [ main:0 ] - [ INFO ]  Obtained 6791 TLD from packaged file tld-names.txt
2016-08-24 17:53:53  [ main:30 ] - [ INFO ]  Deleted contents of: ./crawl_storage/frontier ( as you have configured resumable crawling to false )
2016-08-24 17:53:54  [ main:601 ] - [ INFO ]  Crawler 1 started
2016-08-24 17:53:54  [ Crawler 1:1001 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/: null
2016-08-24 17:53:54  [ Crawler 1:1002 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:70)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 17:54:44  [ Crawler 1:50417 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/yxgj.html: null
2016-08-24 17:54:44  [ Crawler 1:50418 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at org.htmlparser.nodes.TagNode.getAttributeEx(TagNode.java:303)
	at org.htmlparser.nodes.TagNode.getAttribute(TagNode.java:174)
	at spider.util.ParseUtils$1.accept(ParseUtils.java:41)
	at org.htmlparser.nodes.AbstractNode.collectInto(AbstractNode.java:192)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:433)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.tags.CompositeTag.collectInto(CompositeTag.java:435)
	at org.htmlparser.Parser.parse(Parser.java:702)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:33)
	at spider.util.ParseUtils.parseTags(ParseUtils.java:106)
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:76)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 18:05:20  [ main:0 ] - [ INFO ]  Obtained 6791 TLD from packaged file tld-names.txt
2016-08-24 18:05:20  [ main:78 ] - [ INFO ]  Deleted contents of: ./crawl_storage/frontier ( as you have configured resumable crawling to false )
2016-08-24 18:05:21  [ main:1076 ] - [ INFO ]  Crawler 1 started
2016-08-24 18:05:22  [ Crawler 1:1469 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/: null
2016-08-24 18:05:22  [ Crawler 1:1469 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:70)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 18:05:27  [ Crawler 1:7192 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/admin/index.php?action=index%2FtoolSubmit: null
2016-08-24 18:05:27  [ Crawler 1:7192 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:70)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 18:05:41  [ Thread-1:21086 ] - [ INFO ]  It looks like no thread is working, waiting for 10 seconds to make sure...
2016-08-24 18:07:10  [ main:0 ] - [ INFO ]  Obtained 6791 TLD from packaged file tld-names.txt
2016-08-24 18:07:10  [ main:30 ] - [ INFO ]  Deleted contents of: ./crawl_storage/frontier ( as you have configured resumable crawling to false )
2016-08-24 18:07:11  [ main:544 ] - [ INFO ]  Crawler 1 started
2016-08-24 18:07:11  [ Crawler 1:987 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/: null
2016-08-24 18:07:11  [ Crawler 1:988 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:70)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 18:08:42  [ main:0 ] - [ INFO ]  Obtained 6791 TLD from packaged file tld-names.txt
2016-08-24 18:08:42  [ main:32 ] - [ INFO ]  Deleted contents of: ./crawl_storage/frontier ( as you have configured resumable crawling to false )
2016-08-24 18:08:42  [ main:530 ] - [ INFO ]  Crawler 1 started
2016-08-24 18:08:43  [ Crawler 1:1204 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/: null
2016-08-24 18:08:43  [ Crawler 1:1205 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:70)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 18:13:18  [ main:0 ] - [ INFO ]  Obtained 6791 TLD from packaged file tld-names.txt
2016-08-24 18:13:18  [ main:34 ] - [ INFO ]  Deleted contents of: ./crawl_storage/frontier ( as you have configured resumable crawling to false )
2016-08-24 18:13:24  [ main:6005 ] - [ INFO ]  Crawler 1 started
2016-08-24 18:13:24  [ Crawler 1:6555 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/: null
2016-08-24 18:13:24  [ Crawler 1:6556 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:70)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:35)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 18:44:11  [ main:1 ] - [ INFO ]  Obtained 6791 TLD from packaged file tld-names.txt
2016-08-24 18:44:11  [ main:36 ] - [ INFO ]  Deleted contents of: ./crawl_storage/frontier ( as you have configured resumable crawling to false )
2016-08-24 18:44:12  [ main:938 ] - [ INFO ]  Crawler 1 started
2016-08-24 18:44:12  [ Crawler 1:1324 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/: null
2016-08-24 18:44:12  [ Crawler 1:1324 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:76)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:41)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 18:48:00  [ main:0 ] - [ INFO ]  Obtained 6791 TLD from packaged file tld-names.txt
2016-08-24 18:48:00  [ main:37 ] - [ INFO ]  Deleted contents of: ./crawl_storage/frontier ( as you have configured resumable crawling to false )
2016-08-24 18:48:05  [ main:5489 ] - [ INFO ]  Crawler 1 started
2016-08-24 18:48:06  [ Crawler 1:6022 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/: null
2016-08-24 18:48:06  [ Crawler 1:6022 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:77)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:42)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
2016-08-24 18:50:14  [ main:0 ] - [ INFO ]  Obtained 6791 TLD from packaged file tld-names.txt
2016-08-24 18:50:14  [ main:32 ] - [ INFO ]  Deleted contents of: ./crawl_storage/frontier ( as you have configured resumable crawling to false )
2016-08-24 18:50:14  [ main:482 ] - [ INFO ]  Crawler 1 started
2016-08-24 18:50:15  [ Crawler 1:936 ] - [ WARN ]  Unhandled exception while fetching http://hao.199it.com/: null
2016-08-24 18:50:15  [ Crawler 1:936 ] - [ INFO ]  Stacktrace: 
java.lang.NullPointerException
	at spider.ToolsCrawler.BigDataToolsCrawler.fetchToolsInfo(BigDataToolsCrawler.java:77)
	at spider.ToolsCrawler.BigDataToolsCrawler.visit(BigDataToolsCrawler.java:42)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.processPage(WebCrawler.java:421)
	at edu.uci.ics.crawler4j.crawler.WebCrawler.run(WebCrawler.java:274)
	at java.lang.Thread.run(Thread.java:745)
